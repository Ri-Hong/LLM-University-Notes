# Appendix 1.4 How to Convert Text Into Vectors

## One Hot Encoding
Represents each unique word as a vector where the word's position is marked as 1, and all other positions are marked 0. However, it's not memory-efficient and fails to capture word relationships.

## Count Vectorizers
Creates a matrix indicating word frequency in each document. This is similar to one hot encoding but considers word frequencies. Similar cons to One Hot Encoding apply.

## Bag of Words
Uses a dictionary of words to extract features from text. It indicates the presence or absence of dictionary words in the text, disregarding word order.

## N-Grams
Considers sequences of adjacent words or tokens in a sentence. For example, consider the sentence, "I love my dog." If N = 2, then the bigram would be, "I love," "love my", or "my dog" The number of words in each sequence is denoted by N. This method helps capture context.

## TF-IDF
It's a statistical measure that indicates the importance of a word in a document relative to a corpus. It combines term frequency (TF) with inverse document frequency (IDF) to provide weighted values for each word. This method is computationally cost-effective.